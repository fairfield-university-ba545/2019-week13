{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning in Python\n",
    "Learn the basics of deep learning and neural networks along with some fundamental concepts and terminologies used in deep learning.\n",
    "In this tutorial, you will be introduced to the magical world of **deep learning**. This tutorial will mostly cover the basics of deep learning and neural networks. You will learn some fundamental concepts and terminologies used in deep learning, and understand why deep learning techniques are so powerful today. Not only that, but you will also build a simple neural network all by yourself and generate predictions using python's `numpy` library.\n",
    "\n",
    "Tip: For a little background on Artificial Intelligence, Machine Learning & Deep Learning consider reading a post on [Differences Between Machine Learning & Deep Learning](https://www.datacamp.com/community/tutorials/machine-deep-learning) or for an introduction on machine learning [this short tutorial](https://www.datacamp.com/community/tutorials/introduction-machine-learning-python) might be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Background\n",
    "\n",
    "Imagine you work for a loan company, and you need to build a model for predicting whether a user (borrower) should get a loan or not? You have the features for each customer like *age*, *bank balance*, *salary per annum*, *whether retired or not* and so on.\n",
    "\n",
    "<img src = 'https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1544476983/loan_fhzvzp.png' />\n",
    "\n",
    "Consider if you want to solve this problem using a *linear regression* model, then the linear regression will assume that the **prediction** (whether a customer's loan should be sanctioned or not) will be the **sum** of all the features. It will take into account the effect of *age*, *salary*, *bank balance*, *retirement status* and so. So the linear regression model is **not** taking into account **the interaction between these features** or how they affect the overall loan process.\n",
    "\n",
    "<img src = 'https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1544476983/linear_vzbsyb.png' />\n",
    "\n",
    "The above figure left (A) shows prediction from a linear regression model with absolutely no interactions in which it simply adds up the effect of age (`age < 30` or `age > 30`) and bank balance, you can observe from figure (A) that the lack of interaction is reflected by both lines being parallel that is what the linear regression model assumes.\n",
    "\n",
    "On the other hand, figure right (B) shows predictions from a model that allows interactions in which the lines do not have to parallel. Neural Networks is a pretty good modeling approach that allows interactions like the one in figure (B) very well and from these neural networks evolves a term known as **Deep Learning** which uses these powerful neural networks. Because the neural network takes into account these type of interactions so well it can perform quite well on a plethora of prediction problems you have seen till now or possibly not heard.\n",
    "\n",
    "Since neural networks are capable of handling such complex interactions gives them the power to solve challenging problems and do amazing things with\n",
    "\n",
    "- Image\n",
    "- Text\n",
    "- Audio\n",
    "- Video\n",
    "\n",
    "This list is merely a subset of what neural networks are capable of solving, almost anything you can think of in data science field can be solved with neural networks.\n",
    "\n",
    "Deep learning can even learn to write a code for you. Well isn't that super amazing?\n",
    "\n",
    "<img src = 'https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1544476982/hidden_r646yj.png' />\n",
    "\n",
    "The neural network architecture looks something similar to the above figure. On the far left you have the input layer that consists of the features like age, salary per annum, bank balance, etc. and on the far right, you have the output layer that outputs the prediction from the model which in your case is whether a customer should get a loan or not.\n",
    "\n",
    "The layers apart from the input and the output layers are called the hidden layers.\n",
    "\n",
    "Now the question is why they are called **hidden layers**?\n",
    "\n",
    "Well, one good reason is while the input and output layers correspond to apparent things that occur or are present in the world and can be stored as data but the values in the hidden layers are not something that relates to the real world or something for which have data.\n",
    "\n",
    "Technically, each node in the hidden layer represents an aggregation of information from the input data; hence each node adds to the model's capability to capture interactions between the data. The more the nodes, the more interactions can be achieved from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "Let's start by seeing how neural networks use data to make predictions which is taken care by the **forward propagation** algorithm.\n",
    "\n",
    "To understand the concept of forward propagation let's revisit the example of a loan company. For simplification, let's consider only two features as an input namely age and retirement status, the retirement status being a binary ( `0 - not retired` and `1 - retired`) number based on which you will make predictions.\n",
    "\n",
    "<img src = 'https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1544476981/forward_a6lbal.png' />\n",
    "\n",
    "The above figure shows a customer with `age 40` and is `not retired`. The forward propagation algorithm will pass this information through the network/model to predict the output layer. The lines connect each node of the input to every other node of the hidden layer. Each line has a **weight** associated with it which indicates how strongly that feature affects the hidden node connected to that specific line. \n",
    "\n",
    "There are total four weights between input and hidden layer. The first set of weights are connected from the top node of the input layer to the first and second node of the hidden layer; likewise, the second set of weight are connected from the bottom node of the input to the first and second node of the hidden layer.\n",
    "\n",
    "Remember these weights are the key in deep learning which you train or update when you fit a neural network to the data. These weights are commonly known as parameters. \n",
    "\n",
    "__NOTE__: Training a neural network model is learning these weights using the training dataset.\n",
    "\n",
    "To make a prediction for the top node of the hidden layer, you consider each node in the input layer multiply it by the weights connected to that top node and finally sum up all the values resulting in a value `40` (`40 * 1 + 0 * 1 = 40`) as shown in above figure. You repeat the same process for the bottom node of the hidden layer resulting in a value `40`. Finally, for the output layer you follow the same process and obtain a value `0` (`40 * 1 + 40 * (-1) = 0`). This output layer predicts a value `0`.\n",
    "\n",
    "Now you might wonder what the relevance of value `0` is, well you consider the loan problem as binary classification in which an output of `0` indicates a loan sanction and an output of one indicates a loan prohibition.\n",
    "\n",
    "That's pretty much what happens in __forward propagation__. You start from the input layer move to the hidden layer and then to the output layer which then gives you a prediction score. In basic Artificial Neural Network (ANN), you pretty much always use the *multiple-add process*, in linear algebra this operation is a **dot product operation**. In general, a forward propagation is done for a single data point at a time.\n",
    "\n",
    "It's time to see the code for the above forward propagation algorithm!\n",
    "\n",
    "To do this, you will first import a great python library called `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will create a numpy array of `input_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = np.array([40,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the input data, now you will create a dictionary called `weights` in which the keys of the dictionary will hold the variable names for `node0` and `node1` of hidden layers and an output node for the output layer. The values of the dictionary will be the parameters (`weight values`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {'node0':([1,1]),\n",
    "          'node1':([1,-1]),\n",
    "          'output':([1,-1])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly calculate the value of `node0` of the hidden layer. You first multiply the `input_data` with the weights of `node0` and then use a `sum()` function to obtain a scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node0_value = (input_data * weights['node0']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will do the same for the `node1` of the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node1_value = (input_data * weights['node1']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity let's create a numpy array of the `hidden layer values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_layer_values = np.array([node0_value,node1_value])\n",
    "hidden_layer_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you multiply the hidden layer values with the weights of the output layer and again use a `sum()` function to obtain a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = (hidden_layer_values * weights['output']).sum()\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we do the calculation of the figure above using Python - and this is the essentials of **forward propagation** - which means how neural networks function. Note that in this case, we define the weights ourselves - in practice you need a lot of data to train the network in order to learn the values of them.\n",
    "\n",
    "Remember the difference between parameters and hyperparameters? Weights in NN are parameters; hyperparameters would refer to **number of layers**, **number of neurons in a particular layer**, and so on.\n",
    "\n",
    "Deep Learning would not attract this much attention if it can only do above - we can do that with a pen and a piece of paper. Let us move on to the next topic, which makes deep learning so useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "The multiply-add process is only half part of how a neural network works; there's more to it!\n",
    "\n",
    "To utilize the maximum predictive power, a neural network uses an activation function in the hidden layers. An activation function allows the neural network to capture non-linearities present in the data.\n",
    "\n",
    "<img src = 'https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1544476981/act_eabyit.png' />\n",
    "\n",
    "In neural networks, often time the data that you work with is **not** linearly separable (e.g. shapes in an image) and to find a decision boundary that can separate the data points you need some non-linearity in your network. For example, A customer has no previous loan record compared to a customer having a previous loan record may impact the overall output differently. \n",
    "\n",
    "If the relationships in the data aren't straight or linear, then you need a **non-linear activation function** to capture the non-linearity. An activation function is applied to the value coming into a node which then transforms it into the value stored in that node or the node output.\n",
    "\n",
    "$$ y = f(x) $$, \n",
    "\n",
    "in which $y$ is the non-linear output, $x$ is the linear input (it can be non-linear), and $f( )$ is the activation function.\n",
    "\n",
    "Let's apply an s-shaped activation function called $tanh$ to the nodes of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply tanh activation function to both node0 and node1\n",
    "\n",
    "node0_act = np.tanh(node0_value)\n",
    "node1_act = np.tanh(node1_value)\n",
    "\n",
    "# Then apply tanh activation function to the hidden layer as a whole\n",
    "hidden_layer_values_act = np.array([node0_act,node1_act])\n",
    "\n",
    "# Let's look at it\n",
    "hidden_layer_values_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe the difference in the hidden_layer_values (`[40., 40.]`) and hidden_layer_values_act (`[1., 1.]`).\n",
    "\n",
    "Let's quickly calculate the output using the `hidden_layer_values_act`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = (hidden_layer_values_act * weights['output']).sum()\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the output is the same, the meanings are very different. Without the activation function, we are cross-listing two linear regression models, noticed? With the activation function, this is much more than linear regression models.\n",
    "\n",
    "n today's time, an activation function called **Rectifier Linear Unit (ReLU)** is widely used in both industry and research. Even though it has two linear pieces, it's very powerful when combined through multiple hidden layers. **ReLU** is half rectified from the bottom as shown in the figure below.\n",
    "\n",
    "<img src = 'https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1544476981/relu_hp7whu.png' />\n",
    "[Image Source](https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/)\n",
    "\n",
    "Now you will apply **ReLU** as the activation function on the hidden layer nodes and calculate the network's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let's define ReLU first \n",
    "\n",
    "def relu(input):\n",
    "    '''Define your relu activation function here'''\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(0, input)\n",
    "\n",
    "    # Return the value just calculated\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Then let's apply ReLU instead of tanh to above calculations\n",
    "\n",
    "# Calculate node 0 value: node_0_output\n",
    "node_0_input = (input_data * weights['node0']).sum()\n",
    "node_0_output = relu(node_0_input)\n",
    "\n",
    "# Calculate node 1 value: node_1_output\n",
    "node_1_input = (input_data * weights['node1']).sum()\n",
    "node_1_output = relu(node_1_input)\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "# Calculate model output (do not apply relu)\n",
    "model_output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print model output\n",
    "print(\"Model's Output:\",model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, same result, totally different meanings. There are many more activation functions in NNs.\n",
    "\n",
    "Similar to the above example wherein you had just one data point (observation), you can implement the same network for multiple observations or rows of data.\n",
    "\n",
    "## Deep Learning (Deeper Networks)\n",
    "The significant difference between traditional neural networks and the modern deep learning that makes use of neural networks is the use of **not just one but many successive** hidden layers. Research shows that increasing the number of hidden layers **massively improves** the performance making the network capable of more and more interactions.\n",
    "\n",
    "The working in a network with just a single hidden layer and with multiple hidden layers remain the same. You forward propagate through these successive hidden layers as you did in the previous example with one hidden layer.\n",
    "\n",
    "<img src = 'https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1544476981/deep_learning_nxrrbc.png' />\n",
    "[Image Source](https://cdn.edureka.co/blog/wp-content/uploads/2017/05/Deep-Neural-Network-What-is-Deep-Learning-Edureka.png)\n",
    "\n",
    "Let's understand some essential facts about these deep networks!\n",
    "\n",
    "- Deep Learning networks are capable of internally building up representations of the **pattern(s)** in the data that are essential for making accurate predictions;\n",
    "- The patterns in the initial layers are simple (e.g. lines), but as you go through successive hidden layers or deep into the network the network starts learning more and more complex patterns;\n",
    "- Deep learning networks eliminate the need for handcrafted features. You do not need to create better predictive features which you then feed to the deep learning network, the network itself learns meaningful features from the data and using which it makes predictions - in other words, deep learning embeds __feature selection__ in them;\n",
    "- Deep learning is also called Representation Learning since the subsequent layers in the network build increasingly sophisticated representations of the data (which we hope to capture some high-level, abstract features) until you reach to the final layer where it finally makes the prediction.\n",
    "\n",
    "Let's try to get some understanding about the representations that the network tries to capture, from the above figure.\n",
    "\n",
    "The input to the above network is images of humans; You can see that the initial layers in the network are capturing the patterns of local contrast that are conceptually simple, patterns like *vertical edges*, *horizontal*, *diagonal edges*, *blurry areas*, etc. Once the network identifies where are these diagonal or horizontal lines the successive layers then combine that information to find larger patterns like eyes, nose, lips, etc. A much later layer might combine these patterns to find much larger abstract patterns like for example *a face* as depicted in the above figure.\n",
    "\n",
    "Well, the cool thing about deep learning is you don't explicitly tell the network to look for diagonal lines or wherein the image is the nose or a lip, instead of when you train the network the neural network has weights that are learned to find the relevant patterns to make accurate predictions. The learning process in neural networks  is a gradual process in which the network undergoes multiple pieces of training before it can learn to make better predictions. Hence, we train the neural network usually in a recursive fashion.\n",
    "\n",
    "## Forward propagation in a multi-layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## New input data\n",
    "input_data = np.array([3,5])\n",
    "\n",
    "## new weights\n",
    "weights = {'node0_0':([2,4]),\n",
    "          'node0_1':([4,-5]),\n",
    "          'node1_0':([-1,2]),\n",
    "          'node1_1':([1,2]),\n",
    "          'output':([2,7])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's depict the architecture of the NN below.\n",
    "\n",
    "<img src = 'https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1544476981/multilayer_sgpjtz.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_with_network(input_data):\n",
    "    # Calculate node 0 in the first hidden layer\n",
    "    node_0_0_input = (input_data* weights['node0_0']).sum()\n",
    "    node_0_0_output = relu(node_0_0_input)\n",
    "\n",
    "    # Calculate node 1 in the first hidden layer\n",
    "    node_0_1_input = (input_data* weights['node0_1']).sum()\n",
    "    node_0_1_output = relu(node_0_1_input)\n",
    "\n",
    "\n",
    "\n",
    "    # Put node values into array: hidden_0_outputs\n",
    "    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n",
    "    print(\"Hidden Layer 1 Output:\", hidden_0_outputs)\n",
    "\n",
    "    # Calculate node 0 in the second hidden layer\n",
    "    node_1_0_input =  (hidden_0_outputs* weights['node1_0']).sum()\n",
    "    node_1_0_output = relu(node_1_0_input)\n",
    "\n",
    "    # Calculate node 1 in the second hidden layer\n",
    "    node_1_1_input = (hidden_0_outputs* weights['node1_1']).sum()\n",
    "    node_1_1_output = relu(node_1_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_1_outputs\n",
    "    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n",
    "    print(\"Hidden Layer 2 Output:\", hidden_1_outputs)\n",
    "    # Calculate model output: model_output\n",
    "    model_output = (hidden_1_outputs * weights['output']).sum()\n",
    "    # Return model_output\n",
    "    return(model_output)\n",
    "\n",
    "output = predict_with_network(input_data)\n",
    "print(\"Model's Prediction:\",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: If you were able to understand till here then you must consider reading a post on [Convolutional Neural Networks in Python with Keras](https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python) which teaches you a new terminology known as convolutional neural networks, not only that it also uses a deep learning framework called Keras along with a famous image dataset to classify handwritten digits and a lot more.\n",
    "\n",
    "## Go Further!\n",
    "Congratulations to all of you who successfully made it till the end!\n",
    "\n",
    "This post tried to give you an intuitive understanding about the powerful neural networks, how the neural network learns using forward propagation algorithm and how you can implement forward propagation using a python's library called numpy with some facts on the deep neural networks. \n",
    "\n",
    "The learning in neural networks has more components attached to it apart from a forward propagation algorithm so to dive deeper into neural networks and how you can make predictions using this magical network consider taking up BA550 in the summer where we can spend more time looking at how to use deep learning with image or text data.\n",
    "\n",
    "Also, deep learning is one of my current research interests. I have collaborated with students on this topic, and presented our work at premier academic conferences multiple times. Moreover, obtaining an understanding toward deep learning will earn you an edge on the job market. So if you are interested in working on a project like this (e.g. for your capstone project), please let me know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
